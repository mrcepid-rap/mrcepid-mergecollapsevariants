#!/usr/bin/env python
# mrcepid-collapsevariants 0.0.1
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import dxpy
import subprocess
import csv
import json
import pandas as pd
import tarfile
import glob


# This function runs a command on an instance, either with or without calling the docker instance we downloaded
# By default, commands are not run via Docker, but can be changed by setting is_docker = True
def run_cmd(cmd: str, is_docker: bool = False):

    if is_docker:
        # -v here mounts a local directory on an instance (in this case the home dir) to a directory internal to the
        # Docker instance named /test/. This allows us to run commands on files stored on the AWS instance within Docker.
        # This looks slightly different from other versions of this command I have written as I needed to write a custom
        # R script to merge STAAR files. That means we have multiple mounts here to enable this code to find the script.
        cmd = "docker run " \
              "-v /home/dnanexus:/test " \
              "-v /usr/bin/:/prog " \
              "egardner413/mrcepid-associationtesting " + cmd

    # Standard python calling external commands protocol
    print(cmd)
    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = proc.communicate()

    # If the command doesn't work, print the error stream and close the AWS instance out with 'dxpy.AppError'
    if proc.returncode != 0:
        print("The following cmd failed:")
        print(cmd)
        print("STDERROR follows\n")
        print(stderr.decode('utf-8'))
        raise dxpy.AppError("Failed to run properly...")


# Utility function to delete files no longer needed from the AWS instance to save space
def purge_file(file: str) -> None:
    cmd = "rm " + file
    run_cmd(cmd)


# Make REGENIE-specific input files
# As noted below, these are not directly used as we do not run REGENIE. The outputs of this function are
# now used as inputs for other tools
# The primary output of this function is 5 files:
# 1. <file_prefix>.REGENIE.annotation – a tsv of chr / pos / id / gene / annotation of ALL variants being assessed
# 2. <file_prefix>.REGENIE.inclusion - a list of all possible genes (by ENSG) for rare variant testing
# 3. <file_prefix>.REGENIE.setFile - a file of ENSG / chr / pos / variant IDs for all possible genes for rare variant testing
# 4. <file_prefix>.SAIGE.groupFile.txt – similar to setFile but just ENSG / variant IDs for all possible genes for rare variant testing
# 5. <file_prefix>.REGENIE.mask – a file that just contains the "file_prefix"
def merge_REGENIE(input_vcfs: list, file_prefix: str) -> dict:

    # Make a file list for plink to merge all .pgen files into a single pgen file of the entire genome
    # This is formated to be compatible with the --pmerge-list flag of plink2 as shown below. This is just one
    # file per line.
    merge_list = open('merge_list.txt', 'w')
    for vcf in input_vcfs:
        plink_name = vcf + ".REGENIE"
        merge_list.write("/test/" + plink_name + "\n")
    merge_list.close()

    # Run plink2 to merge all .pgen files ingested into this applet into a single .pgen file
    cmd = "plink2 --pmerge-list /test/merge_list.txt --out /test/" + file_prefix + ".REGENIE"
    run_cmd(cmd, True)
    purge_file(file_prefix + ".REGENIE.log")

    # Make a dictionary that will store information about each gene listed in annotation files
    # This will have three pieces of information:
    # 1. Chromosome (chr)
    # 2. Position (poss)
    # 3. Variant IDs (ids) – this field is a list of variant ID strings that include ALL variants within this gene
    genes = {}

    # Here we are both reading in information for each variant from individual vcf.tar.gz files AND outputting the master
    # list of all variants that will be included for rare variant burden testing
    output_annotation = open(file_prefix + ".REGENIE.annotation", "w")
    output_csv = csv.DictWriter(output_annotation, delimiter="\t", fieldnames=['id', 'gene', 'annotation'],
                                extrasaction='ignore')

    # We need to capture the variant type label from 'annotation' to output later
    variant_type = None

    # Loop through each vcf.tar.gz file and:
    # 1. Store individual variant information
    # 2. Output variant information to a list file for rare variant burden testing
    for vcf in input_vcfs:
        annotation_name = vcf + ".REGENIE.annotation.txt"
        annotation_csv = csv.DictReader(open(annotation_name, 'r', newline='\n'), delimiter="\t",
                                        fieldnames=['chr', 'pos', 'id', 'gene', 'annotation'], quoting=csv.QUOTE_NONE)
        # Store all variant information from the above file in the 'genes' dict
        for var in annotation_csv:
            output_csv.writerow(var)
            variant_type = var['annotation']
            if var['gene'] in genes:
                genes[var['gene']]['poss'].append(int(var['pos']))
                genes[var['gene']]['ids'].append(var['id'])
            else:
                genes[var['gene']] = {'chr': var['chr'], 'poss': [int(var['pos'])], 'ids': [var['id']]}

        purge_file(annotation_name) # delete the annotation file to start freeing up space on the instance

    output_annotation.close()

    # Open file handles for output files
    output_inclusion = open(file_prefix + ".REGENIE.inclusion", "w")
    output_setfile_REGENIE = open(file_prefix + ".REGENIE.setfile", "w")
    output_setfile_SAIGE = open(file_prefix + ".SAIGE.groupFile.txt", "w")

    # I know this is goofy, but also just going to do the file for SAIGE here to make it simple
    # We are iterating through all possible genes and writing them to the various output files listed above
    for gene in genes:
        # One file has to literally be just gene IDs we want to analyse
        output_inclusion.write(gene + "\n")

        # find the minimum variant starting position for a given gene and set it as part of the 'gene' dict
        min_pos = min(genes[gene]['poss'])
        genes[gene]['min_poss'] = min_pos

        # This is the inclusion list for variant testing by REGENIE
        output_setfile_REGENIE.write("%s\t%s\t%i\t%s\n" % (gene, genes[gene]['chr'], min_pos, ','.join(genes[gene]['ids'])))

        # SAIGE needs a slightly different ID format, so set that up here and write to our file
        id_string = "\t".join(["{0}:{1}_{2}/{3}".format(*item2) for item2 in [item.replace('chr','').split(":") for item in genes[gene]['ids']]])
        output_setfile_SAIGE.write("%s\t%s\n" % (gene, id_string))

    output_inclusion.close()
    output_setfile_REGENIE.close()
    output_setfile_SAIGE.close()

    # This will be thrown if a variant type wasn't found
    # But otherwise write the one-line file required by REGENIE that states the variant "mask" type
    if variant_type is None:
        raise Exception("Variant type was not found!")
    else:
        output_mask = open(file_prefix + ".REGENIE.mask", "w")
        output_mask.write("Mask1 " + variant_type)
        output_mask.close()

    # Send the genes dictionary back out so we can use for BOLT
    return genes


# Make BOLT-specific input files
# The primary output of this function is a bgen format file (with associated .sample file)
def merge_BOLT(input_vcfs: list, file_prefix: str, REGENIE_genes: dict) -> None:

    # Step 1 is to walk through each .json file created in mrcepid-collapsevariants and mash them together in a loop
    merged_json = {}

    # This is a bad variable name that might get confusing... But this stores the names of the genes in the json file
    # it SHOULD be the same as in REGENIE_genes, but need to be sure
    genes = set()

    # Now loop through each of the individual vcf .json files:
    # These .json files have a structure like:
    # {'sample1': {'eid': '0123456', 'gene1': 1}, 'sample2': {'eid': '1234567'}, 'sample3': {'eid': '2345678', 'gene2': 1}}
    # Note that ALL samples are listed, even if they did not have a qualifying variant
    for vcf in input_vcfs:

        json_file = vcf + ".BOLT.json"
        loaded_json = json.load(open(json_file, 'r')) # This function just loads a json file back into memory as a dict
        if len(merged_json) == 0:
            merged_json = loaded_json # If we are looping the first time through, we don't need to append and can just read directory into memory
            for sample in merged_json: # BUT we do need to get genes out of the first json – shouldn't take long
                for gene in merged_json[sample]:
                    if gene != "eid" and gene not in genes:
                        genes.add(gene)
        else:
            for sample in loaded_json:
                # This is because we store eids within the dictionary, so samples w/vars will have a size > 1, so no
                # need to proceed if the dict only has an eid and nothing else:
                if len(loaded_json[sample]) > 1:
                    # Now iterate through each gene for a given sample...
                    for gene in loaded_json[sample]:
                        # ... and add it to the genes dict we created for documentation purposes
                        if gene != "eid" and gene not in genes:
                            genes.add(gene)

                        # ... and add the sample / gene pair to our final dict
                        if gene != 'eid' and gene in merged_json[sample]:
                            merged_json[sample][gene] += loaded_json[sample][gene]
                        elif gene != 'eid' and gene not in merged_json[sample]:
                            merged_json[sample][gene] = loaded_json[sample][gene]

        purge_file(json_file)  ## clear up storage

    # We have to write this first into plink .ped format and then convert to bgen for input into BOLT
    # We are tricking BOLT here by setting the individual "variants" within bolt to genes. So our map file
    # will be a set of genes, and if an individual has a qualifying variant within that gene, setting it to that value
    output_map = open(file_prefix + '.BOLT.map', 'w')
    output_ped = open(file_prefix + '.BOLT.ped', 'w')
    output_fam = open(file_prefix + '.BOLT.fam', 'w')
    header = ['eid', 'eid'] + list(genes)

    # Make map file (just list of genes with the chromosome and start position of that gene):
    for gene in genes:
        output_map.write("%s %s 0 %i\n" % (REGENIE_genes[gene]['chr'],
                                           gene,
                                           REGENIE_genes[gene]['min_poss']))

    # Make ped / fam files:
    # ped files are coded with dummy genotypes of A A as a ref individual and A C as a carrier
    for sample in merged_json:
        output_ped.write("%s %s 0 0 0 -9 " % (sample, sample))
        output_fam.write("%s %s 0 0 0 -9\n" % (sample, sample))
        genes_processed = 0
        for gene in genes:
            genes_processed += 1 # This is a helper value to make sure we end rows on a carriage return (\n)
            if gene in merged_json[sample]:
                if genes_processed == len(genes):
                    output_ped.write("A C\n")
                else:
                    output_ped.write("A C ")
            else:
                if genes_processed == len(genes):
                    output_ped.write("A A\n")
                else:
                    output_ped.write("A A ")

    output_ped.close()
    output_map.close()
    output_fam.close()

    # And convert to bgen
    # Have to use OG plink to get into .bed format first
    cmd = 'plink --make-bed --file /test/' + file_prefix + '.BOLT --out /test/' + file_prefix + '.BOLT'
    run_cmd(cmd, True)
    # And then use plink2 to make a bgen file
    cmd = "plink2 --export bgen-1.2 'bits='8 --bfile /test/" + file_prefix + ".BOLT --out /test/" + file_prefix + ".BOLT"
    run_cmd(cmd, True)

    # Purge unecessary intermediate files to save space on the AWS instance:
    purge_file(file_prefix + '.BOLT.ped')
    purge_file(file_prefix + '.BOLT.map')
    purge_file(file_prefix + '.BOLT.fam')
    purge_file(file_prefix + '.BOLT.bed')
    purge_file(file_prefix + '.BOLT.bim')
    purge_file(file_prefix + '.BOLT.log')


# Make BOLT-specific input files
# Remember, all we do here is write the bgen file for SAIGE from the REGENIE plink2 files.
# The actual gene-wise information for SAIGE is done above in the REGENIE section
# The primary output of this function is a vcf file and associated .csi index
def merge_SAIGE(file_prefix: str) -> None:

    # Convert the REGENIE pfile that we created into a VCF format file that SAIGE can use
    cmd = "plink2 --pfile /test/" + file_prefix + ".REGENIE --export vcf --out /test/" + file_prefix + ".SAIGE"
    run_cmd(cmd, True)
    purge_file(file_prefix + '.SAIGE.log')

    # bgzip...
    cmd = "bgzip " + file_prefix + ".SAIGE.vcf"
    run_cmd(cmd)

    # ... and index!
    cmd = "bcftools index /test/" + file_prefix + ".SAIGE.vcf.gz"
    run_cmd(cmd, True)


# Make STAAR-specific input files
# The primary output of this function is a single R rds format file of the format <file_prefix>.STAAR.matrix.rds
# The goal of this script is to make inputs that can be provided to a script (at /resources/usr/bin/buildSTAARmatrix.R)
# to make the single matrix for STAAR-based rare variant burden testing
def merge_STAAR(input_vcfs, file_prefix):

    # First we need a sorted set of variants across all VCFs
    iteration_num = 1
    sample_file = None
    variants = [] # This will be a list of pandas data frames that we concat together
    matrix_file_list = open('matrix_files.txt', 'w') # iteratively write a list of matrix files so that we can read them in later

    for vcf in input_vcfs:
        # On the first iteration (iteration_num == 1), take this opportunity to grab a sample file to use for the rows
        # of the merged matrix:
        # I can get this from the REGENIE .psam file
        if iteration_num == 1:
            sample_file = vcf + ".REGENIE.psam"
            iteration_num += 1

        # Now I need to ingest the lists of variants:
        # I can get this from the REGENIE .pvar file from each .tar.gz as we iterate through them
        # Read each in as a pandas dataframe and get the columns we actually care about
        variant_ids = vcf + ".REGENIE.pvar"
        current_vars = pd.read_csv(variant_ids,
                                   sep="\t",
                                   comment="#",
                                   header=None,
                                   names=("chrom", "pos", "ID", "ref", "alt", "qual", "filter", "info"),
                                   index_col=2)
        current_vars = current_vars[["chrom", "pos", "info"]]
        # Process MAF information – pvar files are identical to the first 8 VCF columns. We know that MAF is always the
        # first entry, so I can just do a quick split with number of returns set to '1'
        current_vars[['maf', "extra"]] = current_vars['info'].str.split(';', 1, expand=True)
        # I then just need to strip the "AF" bit off of the resulting MAF information (just use extra again as a garbage bin)
        current_vars[['extra', 'maf']] = current_vars['maf'].str.split('AF=', 1, expand=True)
        # only retain the 3 columns that we need for later, chrom, pos, maf
        current_vars = current_vars[['chrom', 'pos', 'maf']]
        variants.append(current_vars) # add these to our list
        purge_file(variant_ids) # save some space

        matrix_file = vcf + ".STAAR.matrix.txt"
        matrix_file_list.write("/test/" + matrix_file + "\n")  # Have to do '/test/' since this will be run via Docker

    matrix_file_list.close()

    # Deal with merging the variant frames here:
    variants_table = pd.concat(variants) # this just mashes all the dataframes in this list together...
    variants_table = variants_table.sort_values(by=["chrom", "pos"]) # ... and sorts them ...
    variants_table.to_csv(open(file_prefix + '.variants_table.STAAR.tsv', 'w'), sep='\t') # ... and finally outpus to a csv

    # need to make sure we found a sample file. Shouldn't be a problem...
    if sample_file is None:
        raise Exception("Samples file could not be retrieved...")

    # Now run the actual R script to make the final genotype matrix...
    # See the script itself for how it works, but it takes 4 inputs:
    # 1. Samples file (*.psam) – the rows of our final matrix
    # 2. Our list of variants – the columns of our final matrix
    # 3. list of matrix files – the cells in our final matrix
    # 4. prefix – the prefix for the name of the final file
    # And generates one output:
    # 1. A .rds file that can be read back into STAAR during mrcepid-runassociationtesting
    cmd = "Rscript /prog/buildSTAARmatrix.R /test/%s /test/%s /test/%s %s" % \
          (sample_file, file_prefix + '.variants_table.STAAR.tsv', 'matrix_files.txt', file_prefix)
    run_cmd(cmd, True)


@dxpy.entry_point('main')
def main(input_vcf_list, file_prefix):

    # Bring our docker image into our environment so that we can run commands we need:
    cmd = "docker pull egardner413/mrcepid-associationtesting:latest"
    run_cmd(cmd)

    # Ingest the list file into this AWS instance
    dxvcf = dxpy.DXFile(input_vcf_list)
    dxpy.download_dxfile(dxvcf, "vcf_list.txt")

    # Here we are ingesting one "annotation pack" from one VCF file at a time (.tar.gz file) and extracting it.
    # I am unsure how much space this is going to take up to injest ALL VCF files, but likely will be <<100Gb
    # (or even 50)
    # Another concern is adding too many filehandles to the AWS instance to deal with. Currently, we are looking
    # at ~1,000 * 7 = ~7k files in one directory. Should be ok...
    input_vcfs = [] # This is just a list to store the names of all VCF files brought into this
    with open('vcf_list.txt', 'r') as f:
        for line in f:
            line = line.rstrip() # 'line' here is the DNANexus hash of one .tar.gz pack

            # Make a DXFile object out of the file handle and download it
            current_vcf_pack = dxpy.DXFile(line)
            local_name = current_vcf_pack.describe()['name'] # get the actual name of the file
            dxpy.download_dxfile(dxid=current_vcf_pack.get_id(),
                                 filename=local_name)

            # Extract contents of the tarball into the root directory:
            tar = tarfile.open(local_name)
            tar.extractall()
            tar.close()

            # And add the actual name of the VCF file to our list
            input_vcfs.append(local_name.rstrip(".tar.gz"))

    # Here we are generating file inputs for each of the tools that we want to run in mrcepid-runannotationtesting
    # Note that I have a function here for REGENIE – we don't run REGENIE and this is here for legacy purposes
    # because some of the other tools (notably SAIGE) use these files as input so I am keeping for now
    genes = merge_REGENIE(input_vcfs, file_prefix) # REGENIE
    merge_BOLT(input_vcfs, file_prefix, genes) # BOLT
    merge_SAIGE(file_prefix)  # For SAIGE we can use the input for REGENIE to generate bgen files
    merge_STAAR(input_vcfs, file_prefix) # STAAR

    # Purpose here is to generate one tarball of all the information we need for association testing for simple I/O.
    # The only output of this applet is thus a single .tar.gz file per VCF file
    output_tarball = file_prefix + ".tar.gz"
    tar = tarfile.open(output_tarball, "w:gz")
    for file in glob.glob(file_prefix + ".*"):
        tar.add(file)
    tar.close()

    output = {'output_tarball': dxpy.dxlink(dxpy.upload_local_file(output_tarball))}

    return output


dxpy.run()
